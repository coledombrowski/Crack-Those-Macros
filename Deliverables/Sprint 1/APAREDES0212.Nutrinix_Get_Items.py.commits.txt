11975999 (aparedes0212 2024-07-10 20:49:55 -0400   1) from selenium import webdriver
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   2) from selenium.webdriver.firefox.service import Service
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   3) from selenium.webdriver.common.by import By
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   4) from selenium.webdriver.firefox.options import Options
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   5) from bs4 import BeautifulSoup
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   6) import time
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   7) import re
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   8) import random
11975999 (aparedes0212 2024-07-10 20:49:55 -0400   9) import pandas as pd
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  10) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  11) options = Options()
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  12) options.binary_location = r'C:\Program Files\Mozilla Firefox\firefox.exe'
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  13) service = Service(executable_path=r'C:\Users\2cona\OneDrive\Advanced Software Engineering\Local Dev\geckodriver.exe')
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  14) options.add_argument("--headless")
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  15) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  16) # Initialize the WebDriver
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  17) driver = webdriver.Firefox(service=service, options=options)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  18) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  19) # Base URL
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  20) urls = ['https://www.nutritionix.com/brand/mcdonalds/products/513fbc1283aa2dc80c000053',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  21)              'https://www.nutritionix.com/brand/burger-king/products/513fbc1283aa2dc80c00000a',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  22)              'https://www.nutritionix.com/brand/cracker-barrel/products/58ef9673ec6894767f874f4a',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  23)         'https://www.nutritionix.com/brand/wendys/products/513fbc1283aa2dc80c00000f',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  24)         'https://www.nutritionix.com/brand/cook-out/products/57ab8ba0a028b5e1529f4d3e',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  25)         'https://www.nutritionix.com/brand/subway/products/513fbc1283aa2dc80c000005',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  26)         'https://www.nutritionix.com/brand/dominos/products/513fbc1283aa2dc80c000003',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  27)         'https://www.nutritionix.com/brand/taco-bell/products/513fbc1283aa2dc80c000020',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  28)         'https://www.nutritionix.com/brand/buffalo-wild-wings/products/521b95464a56d006cae29dac',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  29)         'https://www.nutritionix.com/brand/waffle-house/products/55ded06e27fbbf82551a8491',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  30)         'https://www.nutritionix.com/brand/applebees/products/513fbc1283aa2dc80c000015',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  31)         'https://www.nutritionix.com/brand/olive-garden/products/513fbc1283aa2dc80c000024',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  32)         'https://www.nutritionix.com/brand/chilis/products/513fbc1283aa2dc80c000022',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  33)         'https://www.nutritionix.com/brand/texas-roadhouse/products/d3ea0422d63a633f2c3eb6f4',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  34)         'https://www.nutritionix.com/brand/chick-fil-a/products/513fbc1283aa2dc80c000025',
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  35)         'https://www.nutritionix.com/brand/scooters-coffee/products/513fbc1283aa2dc80c00061d'
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  36)              ]
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  37) base_urls = [url + "?page=" for url in urls]
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  38) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  39) def get_item_ids(soup):
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  40)     item_ids = []
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  41)     for item in soup.find_all('tr', class_='item-row item-hover ng-scope'):
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  42)         href = item.get('href')
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  43)         if href:
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  44)             item_ids.append(href)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  45)     item_ids = [id.split('/')[-1] for id in item_ids]
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  46)     return item_ids
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  47) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  48) def get_current_page_from_title(soup):
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  49)     title = soup.find('title').text
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  50)     match = re.search(r'Page (\d+)', title)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  51)     if match:
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  52)         return int(match.group(1))
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  53)     return None
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  54) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  55) def main(base_url):
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  56)     # Start from page 1
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  57)     page = 1
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  58)     all_item_ids = []
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  59) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  60)     while True:
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  61)         # Modify the URL to load the appropriate page
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  62)         url = f'{base_url}{page}'
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  63) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  64)         # Load the page
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  65)         driver.get(url)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  66) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  67)         sleep_time_int = random.uniform(2, 6)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  68)         sleep_time_dec = random.uniform(1, 100)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  69)         sleep_time = sleep_time_int + (sleep_time_dec/100)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  70)         time.sleep(sleep_time)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  71) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  72)         # Wait for JavaScript to load
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  73)         driver.implicitly_wait(sleep_time)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  74) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  75)         # Get the page source after JavaScript execution
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  76)         html = driver.page_source
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  77) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  78)         # Parse with BeautifulSoup
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  79)         soup = BeautifulSoup(html, 'html.parser')
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  80) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  81)         current_page_number = get_current_page_from_title(soup)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  82) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  83) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  84)         if current_page_number is None or current_page_number != page:
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  85)             break
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  86) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  87)         current_page_ids = get_item_ids(soup)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  88)         # Add current page items to all items list
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  89)         all_item_ids.extend(current_page_ids)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  90)         print(f'complete page {page}')
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  91) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  92)         # Move to the next page
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  93)         page += 1
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  94)     return all_item_ids
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  95) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  96) # Initialize an empty DataFrame to store the item IDs
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  97) df = pd.DataFrame(columns=['Brand', 'Item_ID'])
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  98) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400  99) # Main loop to process each base URL
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 100) for base_url in base_urls:
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 101)     # Extract the brand name from the URL for labeling purposes in the DataFrame
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 102)     brand_name = base_url.split('/')[4]  # Adjust according to the actual URL format
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 103) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 104)     # Use the main function to get all item IDs for this brand
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 105)     item_ids = main(base_url)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 106) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 107)     # Create a DataFrame from the current brand's item IDs
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 108)     temp_df = pd.DataFrame(item_ids, columns=['Item_ID'])
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 109)     temp_df['Brand'] = brand_name  # Add a column for the brand name
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 110) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 111)     # Append the current brand's DataFrame to the main DataFrame
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 112)     df = pd.concat([df, temp_df], ignore_index=True)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 113) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 114) # Close the browser after processing all URLs
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 115) driver.quit()
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 116) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 117) # Export the DataFrame to a CSV file
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 118) csv_filename = 'item_ids.csv'
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 119) df.to_csv(csv_filename, index=False)
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 120) 
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 121) # Print a message to confirm export
11975999 (aparedes0212 2024-07-10 20:49:55 -0400 122) print(f"Data successfully exported to {csv_filename}")
